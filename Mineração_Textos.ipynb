{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhhVnAMrAduI7KjReigWU2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarolineAndradeR/Data-Science/blob/main/Minera%C3%A7%C3%A3o_Textos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INTRODUÇÃO#\n",
        "\n",
        "Os dados, de forma geral, são armazenados em formato de texto, muitas vezes não estruturados, como respostas de formulários e dados coletados da internet por meio de web scraping. É nesses casos em que utilizamos técnicas de processamento de texto (Text Mining ou mineração de texto) que podemos compreender como uma extensão da mineração de dados, pois envolve o desenvolvimento de modelos, técnicas e ferramentas computacionais para gerar informação útil de texto com base na estrutura morfológica, sintática, semântica e lexical da linguagem, seja ela em formato de texto corrido ou áudio.\n"
      ],
      "metadata": {
        "id": "XeZzUAt-GY23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZAÇÃO**\n",
        "\n",
        "É possível determinar a morfologia do texto, processando as letras maiúsculas e minúsculas, palavras compostas e abreviadas, como também a quebra de caracteres. Ao perceber onde uma sentença começa e termina, podemos separar o texto por tokens.\n",
        "\n",
        "\n",
        "Em vários casos, caracteres como vírgulas, ponto e vírgulas e pontos finais podem ser usados para definir tokens. Por exemplo, no português, as abreviações são sinalizadas com pontos, como Av. (avenida), Sr. ou Sra. (senhor ou senhora) e Dr. ou Dra. (doutor ou doutora)."
      ],
      "metadata": {
        "id": "jbS70Ht9Gu8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma forma simples de criar tokens é por meio do split (fatiamento) de uma lista de palavras."
      ],
      "metadata": {
        "id": "b0fOf4nh84Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importa biblioteca\n",
        "import pandas as pd\n",
        "#abre texto .txt\n",
        "text = pd.read_csv(\"sample.txt\",header=None)\n",
        "#converte um data frame em uma lista de palavras\n",
        "corpus=[]\n",
        "for row in text.values:\n",
        "    tokens = row[0].split(\" \")\n",
        "    for token in tokens:\n",
        "        corpus.append(token)\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arJpremkHE2Z",
        "outputId": "369d279d-3ebb-4703-8290-6576f2e3d67f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['certa', 'manhã', 'O', 'que', 'aconteceu', 'comigo?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK - Uma biblioteca com recursos desenvolvidos para resolver problemas de processamento de linguagem natural."
      ],
      "metadata": {
        "id": "LqdcOn_V8WHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --user nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de5pCGmY8kfB",
        "outputId": "a5b89903-9f33-4002-f412-4b5e1e583ddd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --user feedparser\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "quit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5AEXNUW-gTU",
        "outputId": "28bcaea8-60b2-480c-c542-a36d3f7468e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: feedparser in /root/.local/lib/python3.10/site-packages (6.0.11)\n",
            "Requirement already satisfied: sgmllib3k in /root/.local/lib/python3.10/site-packages (from feedparser) (1.0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com a biblioteca NLTK é possivél usar a técnica de tokenização para dividir strings em listas de substrings, é possivel separar palavras e pontuação de uma sentença. Também pérar mo nível de sentenças, usamdo o tokenizer para separar cada frase do tezto com base em stopwords."
      ],
      "metadata": {
        "id": "mMBS1cYf-xRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importando o corpus\n",
        "import nltk\n",
        "from nltk.corpus import machado\n",
        "#exibindo as stopword do portugues\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "print(\"StopWord: \",stopwords[:30])\n",
        "#tokenização do corpus\n",
        "sent_tokenizer=nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
        "raw_text = machado.raw('romance/marm05.txt')\n",
        "sentences = sent_tokenizer.tokenize(raw_text)\n",
        "#exibindo as frases separadas do Machado de Assis\n",
        "for sent in sentences[1000:1005]:\n",
        "   print(\"<<\", sent, \">>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY8AK201_ZCO",
        "outputId": "ada879a0-3bb5-4b07-f3f1-fc5c02b9bfea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StopWord:  ['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'às', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'é', 'ela', 'elas', 'ele', 'eles']\n",
            "<< Era tarde; a infeliz expirou dentro de alguns\n",
            "segundos. >>\n",
            "<< Fiquei um pouco aborrecido, incomodado. >>\n",
            "<<  Também por que diabo não era ela\n",
            "azul? >>\n",
            "<< disse comigo. >>\n",
            "<< E esta reflexão,  uma das mais\n",
            "profundas que se tem feito, desde a invenção das borboletas,  me consolou do\n",
            "malefício, e me reconciliou comigo mesmo. >>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANÁLISE LÉXICA**\n",
        "\n",
        "A tarefa básica da análise léxica é relacionar variantes morfológicas (variação na grafia da palavra ou na sua forma) aos seus lemmas (que são as formas canônicas das palavras).\n",
        "\n",
        "\n",
        "É nessa etapa que são realizadas a verificações ortográficas e a identificação das partes do texto.\n",
        "\n",
        "\n",
        "Basicamente, uma palavra pode ser pensada como uma sequência de caracteres no texto, a análise léxica relacionará essas sequências de caracteres (palavras) com regras gramaticais e formas canônicas (lemma).\n",
        "\n",
        "Por exemplo: O verbo pegar o lemma será seu radical peg para o conjunto variante morfológicas (pegada, pegando, pega, pegou).\n",
        "\n",
        "Utilizamos a técnica stemming, que se baseia na operação de pré-processamento de textos em que palavras mais complexas morfologicamente são identificadas e decompostas em seu stem invariante, ou melhor, na forma canônica do lemma e seus afixos, e no final os afixos são deletados. O stem, portanto, é o chamado radical da palavra. Utilizando ainda o verbo entregar, para exemplificar, o lemma é entregar e o stem é entreg, pois a partir do stem podem ser criadas outras palavras."
      ],
      "metadata": {
        "id": "s6fie1TE_hjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como pegar o radical de palavras em diferentes idiomas."
      ],
      "metadata": {
        "id": "L_bH-kc6AnQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import *\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "#Criando um novo \"Porter Stemmer\"\n",
        "stemmer = PorterStemmer()\n",
        "#Criando a lista de palavras no plural para testar o stemmer\n",
        "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', 'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', 'traditional', 'reference', 'colonizer','plotted']\n",
        "# Chamando a função que encontra os radicais\n",
        "singles = [stemmer.stem(plural) for plural in plurals]\n",
        "print(\"Radicais das palavras em ingles:\")\n",
        "print(' '.join(singles))\n",
        "# verificando os idiomas disponíveis para utilizar a ferramenta stemmer\n",
        "print(''\"Idiomas disponiveis:\")\n",
        "print(\" \".join(SnowballStemmer.languages))\n",
        "# trocando o idioma para português\n",
        "stemmer2 = SnowballStemmer(\"portuguese\", ignore_stopwords=True)\n",
        "print(\"Radicais das palavras em portugues:\")\n",
        "print(stemmer2.stem(\"entregar\"))\n",
        "print(stemmer2.stem(\"livre\"))\n",
        "print(stemmer2.stem(\"feliz\"))\n",
        "print(stemmer2.stem(\"infeliz\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmOsjKZhAty2",
        "outputId": "4a6faf6a-2ef9-4728-a8a7-d79ed0877c4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Radicais das palavras em ingles:\n",
            "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n",
            "Idiomas disponiveis:\n",
            "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n",
            "Radicais das palavras em portugues:\n",
            "entreg\n",
            "livr\n",
            "feliz\n",
            "infeliz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANÁLISE SINTÁTICA**\n",
        "\n",
        "Também conhecida como **parsing**, tem a função de extrair informações de uma sentença por meio da gramática e das estruturas de árvores sintáticas. O objetivo principal do parsing é analisar e gerar sentenças corretas de acordo com a estrutura de cada palavra."
      ],
      "metadata": {
        "id": "S5U-O2GaA9FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CRIANDO UMA ÁRVORE SINTATÁTICA COM NLTK\n",
        "#importando as libs utilizadas\n",
        "import nltk\n",
        "from nltk.tree import *\n",
        "from nltk.corpus import wordnet as wn\n",
        "#criando as árvores para teste\n",
        "dp1 = Tree('dp', [Tree('d', ['o']), Tree('np', ['cachorro'])])\n",
        "dp2 = Tree('dp', [Tree('d', ['o']), Tree('np', ['gato'])])\n",
        "vp = Tree('vp', [Tree('v', ['perseguiu']), dp2])\n",
        "tree = Tree('s', [dp1, vp])\n",
        "print(tree)\n",
        "#acessando a label do nó usando o método label()\n",
        "dp1.label(), dp2.label(), vp.label(), tree.label()\n",
        "#O método treepositions retorna uma lista das posições da árvore de sub árvores e folhas em uma árvore. Por padrão, ele fornece a posição de cada árvore, subárvore e folha, ordenando pelo prefixo\n",
        "print(tree.treepositions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WSmf0lrBXX1",
        "outputId": "329d7525-2dd6-40ad-9a23-0d6818d9fcf1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(s (dp (d o) (np cachorro)) (vp (v perseguiu) (dp (d o) (np gato))))\n",
            "[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), (1, 1), (1, 1, 0), (1, 1, 0, 0), (1, 1, 1), (1, 1, 1, 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXIBINDO A ÁRVORE SINTÁTICA EM DIFERENTES FORMATOS\n",
        "#Além de str e repr, existem vários métodos para converter um objeto de árvore em uma das várias codificações de árvores possíveis:\n",
        "print(\"Árvore em Formato latex\")\n",
        "print(tree.pformat_latex_qtree())\n",
        "print(\"Árvore em formato ASCII\")\n",
        "print(tree.pretty_print())\n",
        "print(\"Árvore no formato de chaves\")\n",
        "print(tree.pretty_print(unicodelines=True, nodedist=4))\n",
        "#O método de classe Tree.fromlist() pode ser usado para analisar árvores expressas como listas aninhadas, como aquelas produzidas pela função tree() do módulo wordnet.\n",
        "t=Tree.fromlist(wn.synset('dog.n.01').tree(lambda s:s.hypernyms()))\n",
        "print(\"Método Tree.formlis()\")\n",
        "print(t.pretty_print())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiAzo_dZBg_V",
        "outputId": "f9e415e1-ae98-4975-d571-7c761ff887ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Árvore em Formato latex\n",
            "\\Tree [.s\n",
            "        [.dp [.d o ] [.np cachorro ] ]\n",
            "        [.vp [.v perseguiu ] [.dp [.d o ] [.np gato ] ] ] ]\n",
            "Árvore em formato ASCII\n",
            "                     s                 \n",
            "      _______________|______            \n",
            "     |                      vp         \n",
            "     |                ______|___        \n",
            "     dp              |          dp     \n",
            "  ___|_____          |       ___|___    \n",
            " d         np        v      d       np \n",
            " |         |         |      |       |   \n",
            " o      cachorro perseguiu  o      gato\n",
            "\n",
            "None\n",
            "Árvore no formato de chaves\n",
            "                              s                          \n",
            "        ┌─────────────────────┴─────────┐                    \n",
            "        │                               vp               \n",
            "        │                     ┌─────────┴──────┐             \n",
            "        dp                    │                dp        \n",
            " ┌──────┴────────┐            │         ┌──────┴──────┐      \n",
            " d               np           v         d             np \n",
            " │               │            │         │             │      \n",
            " o            cachorro    perseguiu     o            gato\n",
            "\n",
            "None\n",
            "Método Tree.formlis()\n",
            "                  Synset('dog.n.01')                  \n",
            "         _________________|__________________          \n",
            "Synset('canine.n.                            |        \n",
            "       02')                                  |        \n",
            "        |                                    |         \n",
            " Synset('carnivor                            |        \n",
            "     e.n.01')                                |        \n",
            "        |                                    |         \n",
            " Synset('placenta                            |        \n",
            "     l.n.01')                                |        \n",
            "        |                                    |         \n",
            "Synset('mammal.n.                            |        \n",
            "       01')                                  |        \n",
            "        |                                    |         \n",
            " Synset('vertebra                            |        \n",
            "    te.n.01')                                |        \n",
            "        |                                    |         \n",
            "Synset('chordate.                     Synset('domestic\n",
            "      n.01')                           _animal.n.01') \n",
            "        |                                    |         \n",
            "Synset('animal.n.                    Synset('animal.n.\n",
            "       01')                                 01')      \n",
            "        |                                    |         \n",
            "Synset('organism.                    Synset('organism.\n",
            "      n.01')                               n.01')     \n",
            "        |                                    |         \n",
            " Synset('living_t                     Synset('living_t\n",
            "   hing.n.01')                          hing.n.01')   \n",
            "        |                                    |         \n",
            " Synset('whole.n.                     Synset('whole.n.\n",
            "       02')                                 02')      \n",
            "        |                                    |         \n",
            "Synset('object.n.                    Synset('object.n.\n",
            "       01')                                 01')      \n",
            "        |                                    |         \n",
            " Synset('physical                     Synset('physical\n",
            "  _entity.n.01')                       _entity.n.01') \n",
            "        |                                    |         \n",
            "Synset('entity.n.                    Synset('entity.n.\n",
            "       01')                                 01')      \n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANÁLISE SEMÂNTICA**\n",
        "\n",
        "É a área de pesquisa dentro de data mining que se dedica a estudar o reconhecimento de palavras considerando o contexto. O contexto é uma peça-chave para decodificar a linguagem, considerando a complexidade da comunicação humana. Uma frase pode mudar completamente a sua conotação dependendo do contexto em que ela está inserida. Para determinar com precisão a informação numa frase, é preciso considerar o contexto.\n",
        "\n",
        "\n",
        "A semântica pode ser dividida em léxica e gramatical, em que a primeira busca uma representação conceitual para descrever o sentido. Para construir essa representação, podemos decompor as palavras em unidades léxicas, onde pegamos o menor conjunto de caracteres possível para representar aquela informação, e partimos daí. Podemos também utilizar as redes semânticas e procura identificar o sentido por meio de uma fórmula lógico-semântica. Porém, pode ocorrer ambiguidade."
      ],
      "metadata": {
        "id": "6Tw9MI0aBuvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Como mapear a semântica de um fragmento de texto\n",
        "\n",
        "#importando o corpus\n",
        "import nltk\n",
        "from nltk.corpus import floresta\n",
        "#exibindo o dataset retirado de https://www.linguateca.pt/Floresta/\n",
        "print(\"Corpus sem tratamento\",floresta.words())\n",
        "#Exibindo o corpus com informações sintáticas\n",
        "print(\"Corpus com sinalização Sintática\",floresta.tagged_words())\n",
        "\n",
        "#simplificando as informações sintáticas\n",
        "def simplify_tag(t):\n",
        "    if \"+\" in t:\n",
        "        return t[t.index(\"+\")+1:]\n",
        "    else:\n",
        "        return t\n",
        "twords = floresta.tagged_words()\n",
        "twords = [(w.lower(), simplify_tag(t)) for (w,t) in twords]\n",
        "print(\"Corpus com as sinalizações simplificadas\",twords[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_At4QGsCr8z",
        "outputId": "bd2b91f4-3d3a-4368-db9d-1225318e8405"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus sem tratamento ['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]\n",
            "Corpus com sinalização Sintática [('Um', '>N+art'), ('revivalismo', 'H+n'), ...]\n",
            "Corpus com as sinalizações simplificadas [('um', 'art'), ('revivalismo', 'n'), ('refrescante', 'adj'), ('o', 'art'), ('7_e_meio', 'prop'), ('é', 'v-fin'), ('um', 'art'), ('ex-libris', 'n'), ('de', 'prp'), ('a', 'art'), ('noite', 'n'), ('algarvia', 'adj'), ('.', '.'), ('é', 'v-fin'), ('uma', 'num'), ('de', 'prp'), ('as', 'art'), ('mais', 'adv'), ('antigas', 'adj'), ('discotecas', 'n'), ('de', 'prp'), ('o', 'art'), ('algarve', 'prop'), (',', ','), ('situada', 'v-pcp'), ('em', 'prp'), ('albufeira', 'prop'), (',', ','), ('que', 'pron-indp'), ('continua', 'v-fin'), ('a', 'prp'), ('manter', 'v-inf'), ('os', 'art'), ('traços', 'n'), ('decorativos', 'adj'), ('e', 'conj-c'), ('as', 'art'), ('clientelas', 'n'), ('de', 'prp'), ('sempre', 'adv'), ('.', '.'), ('é', 'v-fin'), ('um_pouco', 'adv'), ('a', 'art'), ('versão', 'n'), ('de', 'prp'), ('uma', 'art'), ('espécie', 'n'), ('de', 'prp'), ('«', '«'), ('outro', 'pron-det'), ('lado', 'n'), ('de', 'prp'), ('a', 'art'), ('noite', 'n'), (',', ','), ('a', 'prp'), ('meio', 'adj'), ('caminho', 'n'), ('entre', 'prp'), ('os', 'art'), ('devaneios', 'n'), ('de', 'prp'), ('uma', 'art'), ('fauna', 'n'), ('periférica', 'adj'), (',', ','), ('seja', 'v-fin'), ('de', 'prp'), ('lisboa', 'prop'), (',', ','), ('londres', 'prop'), (',', ','), ('dublin', 'prop'), ('ou', 'conj-c'), ('faro', 'n'), ('e', 'conj-c'), ('portimão', 'prop'), (',', ','), ('e', 'conj-c'), ('a', 'art'), ('postura', 'n'), ('circunspecta', 'adj'), ('de', 'prp'), ('os', 'art'), ('fiéis', 'n'), ('de', 'prp'), ('a', 'art'), ('casa', 'n'), (',', ','), ('que', 'pron-indp'), ('de', 'prp'), ('ela', 'pron-pers'), ('esperam', 'v-fin'), ('a', 'art'), ('música', 'n'), ('«', '«'), ('geracionista', 'n'), ('de', 'prp'), ('os', 'art')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANÁLISE PRAGMÁTICA**\n",
        "\n",
        "Busca num conjunto de frases a compreensão de um discurso por meio da análise e da coerência textual. As estruturas mais utilizadas na análise pragmática são as gramáticas baseadas em casos, que são gramáticas semânticas não-terminais que formam padrões e podem ser reconhecidas dentro de um contexto.\n",
        "\n",
        "Um algoritmo utilizado para análise pragmática atribui pesos aos pronomes encontrados em frase, esses pesos são chamados de “valores de saliência”. Quanto mais próximo o pronome estiver do sujeito, maior será o valor a ele atribuído como referência aos pronomes. Depois, é aplicado um filtro de acordo com o gênero e número, também são levadas em conta outras características como o tipo do pronome e se a frase tem objeto indireto ou advérbio."
      ],
      "metadata": {
        "id": "NyPueBqZDFPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONSTRUINDO UM CLASSIFICADOR DE NOME POR GÊNERO COM NLTK**\n",
        "\n",
        "Nomes masculinos e femininos possuem características distintas, nomes que terminam com a vogal a, e, e a vogal i, provavelmente são do gênero feminino, enquanto os nomes que terminam em k, o, r, s e t provavelmente são do sexo masculino.\n"
      ],
      "metadata": {
        "id": "OE-GfjduDjpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importando os módulos\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "#Vamos criar uma função para filtrar os significantes de gênero\n",
        "def gender_features(name):\n",
        "   name = name.lower()\n",
        "   return {\n",
        "       'ultimo_caractere': name[-1],\n",
        "       'ultimos_dois': name[-2:],\n",
        "       'ultimos_tres': name[-3:],\n",
        "       'primeiro_caractere': name[0],\n",
        "       'segundo_caractere': name[:1]\n",
        "   }\n",
        "\n",
        "#Vamos preparar os a lista de nomes para serem classificados\n",
        "f_names = nltk.corpus.names.words('female.txt')\n",
        "m_names = nltk.corpus.names.words('male.txt')\n",
        "\n",
        "#Em seguida, usamos o extrator de recursos para processar os nomes e dividir a lista resultante em um conjunto de treinamento e um conjunto de testes.\n",
        "all_names = [(i, 'masculino') for i in m_names] + [(i, 'feminino') for i in f_names]\n",
        "random.shuffle(all_names)\n",
        "test_set = all_names[500:]\n",
        "train_set= all_names[:500]\n",
        "\n",
        "test_set_feat = [(gender_features(n), g) for n, g in test_set]\n",
        "train_set_feat= [(gender_features(n), g) for n, g in train_set]\n",
        "\n",
        "# O conjunto de treinamento é usado para treinar um novo classificador do tipo \"naive Bayes\".\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set_feat)\n",
        "\n",
        "print(\"Maria é do genero: \",classifier.classify(gender_features('Maria')))\n",
        "\n",
        "print(\"João é do gênero: \",classifier.classify(gender_features('João')))\n",
        "\n",
        "# Observe que esses nomes estão classificados corretamente.\n",
        "#Por fim, podemos examinar o classificador para determinar quais recursos ele achou mais eficazes para distinguir os gêneros dos nomes\n",
        "# print (classifier.show_most_informative_features(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILbYXEMqDvPD",
        "outputId": "c759df8f-6106-4b3c-ba77-0f3e68450049"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maria é do genero:  feminino\n",
            "João é do gênero:  masculino\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROCESSAMENTO DE TEXTO**\n",
        "\n",
        "Ao processar texto, é essencial identificar relações implícitas e explícitas nos dados para obter conhecimento que apoie decisões. O resultado deve ser compreensível, útil e relevante para os usuários finais.\n",
        "\n",
        "O que é necessário descobrir:\n",
        "- Como podemos escrever programas para acessar texto de arquivos locais e da web?\n",
        "\n",
        "- Como podemos dividir documentos em palavras individuais e símbolos de pontuação, para que possamos realizar os mesmos tipos de análise que fizemos com corpus de texto nos tópicos anteriores?\n",
        "\n",
        "- Como podemos escrever programas para produzir saída formatada e salvá-la em um arquivo?"
      ],
      "metadata": {
        "id": "ooOUHu-A_itG"
      }
    },
    {
      "source": [
        "# importando as libs utilizadas\n",
        "import nltk\n",
        "from urllib import request\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Baixando os dados necessários para a tokenização\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "#definindo a URL que contém o livro \"A Relíquia\" de Eça de Queirós\n",
        "url = \"https://www.gutenberg.org/cache/epub/17515/pg17515.txt\"\n",
        "#Fazendo a requisição do livro definindo disponível no acervo online\n",
        "response = request.urlopen(url)\n",
        "#Decodificando o corpus para o formato UTF8\n",
        "raw = response.read().decode('utf8')\n",
        "#criando a lista de tokens\n",
        "tokens = word_tokenize(raw)\n",
        "#Exibindo as informações do arquivo\n",
        "print(\"Tipo de dado dos tokens\",type(tokens))\n",
        "print(\"Tamanho do corpus do token\", len(tokens))\n",
        "print(\"Uma fatia do conteúdo do token\", tokens[:12])"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47m2fM3gA8My",
        "outputId": "24e9a214-ca05-43e0-fd6d-391f58c468bf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tipo de dado dos tokens <class 'list'>\n",
            "Tamanho do corpus do token 103361\n",
            "Uma fatia do conteúdo do token ['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'A', 'Relíquia', 'This', 'ebook', 'is', 'for', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importando as libs utilizadas\n",
        "import nltk\n",
        "from urllib import request\n",
        "from nltk.tokenize import word_tokenize\n",
        "#definindo a URL que contém o livro \"A Relíquia\" de Eça de Queirós\n",
        "url = \"https://www.gutenberg.org/cache/epub/17515/pg17515.txt\"\n",
        "#Fazendo a requisição do livro definindo disponível no acervo online\n",
        "response = request.urlopen(url)\n",
        "#Decodificando o corpus para o formato UTF8\n",
        "raw = response.read().decode('utf8')\n",
        "#criando a lista de tokens\n",
        "tokens = word_tokenize(raw)\n",
        "#Criando um corpus NLTK\n",
        "text = nltk.Text(tokens)\n",
        "#Exibindo as informações do arquivo\n",
        "print(\"Tipo de dado dos tokens\",type(tokens))\n",
        "print(\"Tipo de dado dos text\",type(text))\n",
        "print(\"Uma fatia do conteúdo do token\", tokens[150:200])\n",
        "print(\"Exibindo o corpus criado\", text[150:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnmH2ZnQBS41",
        "outputId": "a360dd1b-9d11-49d8-ae23-9576b6737b1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tipo de dado dos tokens <class 'list'>\n",
            "Tipo de dado dos text <class 'nltk.text.Text'>\n",
            "Uma fatia do conteúdo do token ['Biblioteca', 'Nacional', 'de', 'Portugal', ')', '.', ')', '*', '*', '*', 'START', 'OF', 'THE', 'PROJECT', 'GUTENBERG', 'EBOOK', 'A', 'RELÍQUIA', '*', '*', '*', 'Produced', 'by', 'Rita', 'Farinha', 'and', 'the', 'Online', 'Distributed', 'Proofreading', 'Team', 'at', 'http', ':', '//www.pgdp.net', '(', 'This', 'file', 'was', 'produced', 'from', 'images', 'generously', 'made', 'available', 'by', 'National', 'Library', 'of', 'Portugal']\n",
            "Exibindo o corpus criado ['Biblioteca', 'Nacional', 'de', 'Portugal', ')', '.', ')', '*', '*', '*', 'START', 'OF', 'THE', 'PROJECT', 'GUTENBERG', 'EBOOK', 'A', 'RELÍQUIA', '*', '*', '*', 'Produced', 'by', 'Rita', 'Farinha', 'and', 'the', 'Online', 'Distributed', 'Proofreading', 'Team', 'at', 'http', ':', '//www.pgdp.net', '(', 'This', 'file', 'was', 'produced', 'from', 'images', 'generously', 'made', 'available', 'by', 'National', 'Library', 'of', 'Portugal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROCESSANDO FEED RSS**\n",
        "\n",
        "Os blogs são uma valiosa fonte de textos formais e informais. Com a biblioteca Python Universal Feed Parser, é possível acessar o conteúdo de um blog facilmente."
      ],
      "metadata": {
        "id": "_TH9FZxnBbKy"
      }
    },
    {
      "source": [
        "!pip install feedparser"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFal1xiWB7k4",
        "outputId": "21a0b0e3-5891-49e7-d435-4b4bc9ee2573"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=5c50006e3c21cf36bd7f3a45eae0322cd68be05909862ac77f17344103cdb639\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importando as libs utilizadas\n",
        "import nltk\n",
        "from urllib import request\n",
        "from nltk.tokenize import word_tokenize\n",
        "import feedparser\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#definindo a URL do blog que queremos capturar\"\n",
        "feed_url =\"http://noticias.gov.br/noticias/rss\"\n",
        "#determinando o conteúdo que é interessante para nosso código\n",
        "blog_feed = feedparser.parse(feed_url)\n",
        "blog_feed.feed.title\n",
        "blog_feed.feed.link\n",
        "len(blog_feed.entries)\n",
        "print(blog_feed.entries[0].title)\n",
        "print(blog_feed.entries[0].link)\n",
        "print(blog_feed.entries[0].published)\n",
        "content = blog_feed.entries[7].description\n",
        "print(\"Exibindo o conteúdo capturado\",content[:70])\n",
        "raw = BeautifulSoup(content, 'html.parser').get_text()\n",
        "#aplicando tokenização ao conteúdo do capturado do blog\n",
        "print(\"Tokenização do conteudo\",word_tokenize(raw))"
      ],
      "metadata": {
        "id": "vWXrKpp2CXCz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}